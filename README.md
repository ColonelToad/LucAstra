# LucAstra

**An augmented operating system built in Rust with embedded LLM and agentic capabilities.**

LucAstra is a prototype operating system that deeply integrates language models for natural language interaction, semantic search, and autonomous task execution via tools. Supports local inference (llamafile) and cloud APIs (OpenAI) with multi-turn conversations.

## ðŸ“¢ Version 1.1.0 - Async LLM + Vector Search

**LucAstra v1.1.0 adds async LLM providers with vector-based semantic search!**

### New in v1.1.0
- **ðŸ”„ Async LLM Provider Abstraction**: Unified interface for OpenAI, llamafile, and future providers
- **ðŸ§  OpenAI Integration**: GPT-4o-mini completions + text-embedding-3-small (1536-dim vectors)
- **ðŸ” Vector Similarity Search**: Cosine-based semantic search replacing keyword-only BM25
- **ðŸ’¬ Conversation Management**: Multi-turn context with automatic windowing
- **ðŸ“Š 81 Tests Passing** (+11 new tests for providers, vectors, conversations)

### Previous Release (v1.0.0)
- **Phase 1**: HostFileAccess, audit logging, SecurityConfig (read/write/USB)
- **Phase 2**: relibc syscall handler, ELF loader, LibreOffice launcher
- **Phase 3**: Calculator + File Manager apps
- **Phase 4**: Lightweight browser (HTTP, HTML parsing, tabs, bookmarks)
- **Phase 5**: Observability (structured logging, metrics, integration tests)
- **Phase 6**: Release engineering (semver, CI/CD, Docker, packaging)

## âœ¨ Current Features

### Core OS
- âœ… Kernel boot and lifecycle management
- âœ… Hardware Abstraction Layer (HAL) with pluggable device drivers
- âœ… Device manager (USB, input devices)
- âœ… Filesystem manager with mount/unmount support
- âœ… Input event management
- âœ… Tracing and observability throughout

### AI & Search (v1.1.0)
- âœ… **Async LLM Providers**: OpenAI (GPT-4o-mini), llamafile (local 7B models)
- âœ… **Vector Embeddings**: OpenAI text-embedding-3-small (1536 dimensions)
- âœ… **Semantic Search**: Cosine similarity-based vector index
- âœ… **BM25 Keyword Search**: Full-text search with TF-IDF scoring
- âœ… **Conversation Management**: Multi-turn context with automatic windowing
- âœ… **RAG Pipeline**: Retrieval-Augmented Generation with context injection
- âœ… Graceful fallback to mock responses when LLM offline

### Compatibility Layer
- âœ… Relibc syscall handler (20+ syscalls)
- âœ… File descriptor table management
- âœ… FAT32 boot sector parser
- âœ… ELF header parser and validator

### GUI & Tools
- âœ… Desktop-style GUI with chat interface (iced)
- âœ… Taskbar with file manager button
- âœ… Color-coded chat messages
- âœ… Scrollable message history
- âœ… Search tool (BM25 filesystem search)
- âœ… Read tool (file contents)
- âœ… Install tool (execute commands, install programs)
- âœ… Tool execution framework for agentic tasks


## ðŸš€ Quick Start (developers)

### Run the GUI
```powershell
cargo run --package lucastra-gui
```

What it does: boots kernel, initializes services, scans devices, mounts filesystem, indexes documents, and shows the chat interface.

### Run the CLI
```powershell
cargo run --package lucastra-app
```

### Try the tool demo
```powershell
cargo run --package lucastra-app --example tool_demo
```

### Configuration

#### LLM Provider Setup (v1.1.0)

**Option 1: OpenAI (Cloud)**
```bash
# Set API key
export OPENAI_API_KEY=sk-...

# Create config with OpenAI provider
cat > ~/.lucastra/config.json <<EOF
{
  "llm": {
    "provider": "openai",
    "api_key": "\${OPENAI_API_KEY}",
    "model": "gpt-4o-mini",
    "temperature": 0.7,
    "max_tokens": 4096
  },
  "embeddings": {
    "provider": "openai",
    "model": "text-embedding-3-small"
  }
}
EOF
```

**Option 2: Llamafile (Local)**
```bash
# Start llamafile server
llamafile -m mistral-7b-instruct.Q4_K_M.gguf --server --port 8000

# Use llamafile config
export LUCASTRA_CONFIG_HOME=./docs/examples/configs/dev.json
```

See [docs/examples/v1_1_async_llm.rs](docs/examples/v1_1_async_llm.rs) for complete examples.

#### Environment Variables
- **LUCASTRA_CONFIG_HOME**: Root directory for config.json (default: `~/.lucastra`)
- **OPENAI_API_KEY**: OpenAI API key (for OpenAI provider)
- **RUST_LOG**: Log level override (optional, respects config setting)

Logs live in `./logs` (file + console). Audit logs are JSON lines in `./audit/` when file operations occur.

## ðŸ“¦ Project Structure

```
LucAstra/
â”œâ”€â”€ kernel/         Boot coordination and lifecycle management
â”œâ”€â”€ core/           Shared types (Command, Response, Error)
â”œâ”€â”€ services/       Service registry framework
â”œâ”€â”€ hal/            Hardware Abstraction Layer (device traits)
â”œâ”€â”€ devices/        Device enumeration and management
â”œâ”€â”€ fs/             Filesystem mounting and I/O routing
â”œâ”€â”€ input/          Input event buffering
â”œâ”€â”€ llm/            LLM integration (llamafile HTTP client)
â”œâ”€â”€ search/         BM25 search service
â”œâ”€â”€ compat/         Linux compatibility (relibc syscalls, ELF loader)
â”œâ”€â”€ tools/          Agentic tools (search, read, install)
â”œâ”€â”€ app/            CLI + library for system orchestration
â”œâ”€â”€ gui/            Desktop GUI (iced)
â””â”€â”€ db/             Database abstractions (future: LanceDB)
```

## ðŸ› ï¸ Development

### Prerequisites
- Rust 1.90+ 
- PowerShell (Windows) or Bash (Linux/Mac)
- Optional: llamafile for LLM inference

### Building
```powershell
# Build entire workspace
cargo build --workspace

# Build specific crate
cargo build --package lucastra-gui

# Run tests
cargo test --workspace --lib
```

### Code Quality
```powershell
cargo fmt
cargo clippy --workspace
```

### LLM Setup (Optional)
1. Download llamafile (7B model)
2. Start server: `llamafile --server --port 8000`
3. LucAstra will connect automatically (falls back to mock if offline)

## ðŸ“š Documentation

- **[docs/OS_ARCHITECTURE.md](docs/OS_ARCHITECTURE.md)** - System design and architecture
- **[docs/MVP_SUMMARY.md](docs/MVP_SUMMARY.md)** - MVP completion checklist
- **[docs/GUI_TOOLS_GUIDE.md](docs/GUI_TOOLS_GUIDE.md)** - GUI usage and tool API reference
**Release Documentation**:
- **[docs/CHANGELOG.md](docs/CHANGELOG.md)** - Complete feature history (v0.1.0 â†’ v1.0.0)
- **[docs/RELEASE_NOTES.md](docs/RELEASE_NOTES.md)** - Deployment guide with configuration setup
- **[docs/CONFIG_SCHEMA.md](docs/CONFIG_SCHEMA.md)** - Configuration reference and best practices
- **[docs/PHASE6_SUMMARY.md](docs/PHASE6_SUMMARY.md)** - Release engineering completion summary


## ðŸ§ª Testing

All core functionality has been tested:
- âœ… Device enumeration (3 mock devices)
- âœ… Filesystem mounting and I/O
- âœ… BM25 search with document indexing
- âœ… RAG pipeline (search â†’ context â†’ LLM)
- âœ… Syscall handler (20+ syscalls)
- âœ… FAT32 and ELF parsing
- âœ… Tool execution (search, read, install)
- âœ… GUI chat interface
**v1.0.0 Test Results**:
- âœ… **68 Tests Total** (all passing)
	- 53 unit tests (calculator, file manager, browser, security, observability)
	- 15 integration tests (system state, metrics, RAG pipeline, file access)
- âœ… **100% Pass Rate** on Windows and Linux
- âœ… **Security Audit**: Clean dependency scan
- âœ… **Code Quality**: Clippy and rustfmt compliant

Run tests locally:
```bash
cargo test --lib          # Unit tests only
cargo test --test '*'     # Integration tests only
cargo test --all          # All tests
```


## ðŸŽ¯ Roadmap

### v0.2 (Next)
- [ ] Real device drivers (USB, keyboard, mouse)
- [ ] Persistent filesystem (beyond mock)
- [ ] Tool chaining and automation
- [ ] Permission system for tools
- [ ] Enhanced file manager GUI

### v0.3 (Future)
- [ ] LanceDB integration for vector search
- [ ] Run LibreOffice via relibc
- [ ] Custom tool API for user extensions
- [ ] Multi-window GUI support
- [ ] Real-time system monitoring

### v1.0 (Vision)
- [ ] Full Linux app compatibility
- [ ] Distributed LLM inference
- [ ] Plugin ecosystem
- [ ] Production-ready security
- [ ] Native hardware boot (no host OS)
**Current Status: v1.0.0 Production Release âœ…**

The roadmap above represents vision for v1.1, v1.2, and beyond. All core v1.0 features are complete and production-ready.


## ðŸ¤ Contributing

LucAstra is an experimental project. Contributions welcome!

1. Fork the repository
2. Create a feature branch
3. Make your changes with tests
4. Run `cargo fmt` and `cargo clippy`
5. Submit a pull request
## ðŸ¢ Build & Deployment

### CI/CD Pipeline
GitHub Actions automatically:
- Lint code (rustfmt, clippy) on every push
- Build release binaries for Windows and Linux
- Run full test suite (68 tests)
- Scan dependencies for security vulnerabilities
- Generate code coverage reports
- Create releases on version tags

Pipeline configuration: `.github/workflows/ci.yml`

### Release Artifacts
Each release includes:
- Compiled binaries (Windows `.exe`, Linux binary)
- Source code
- Documentation and configuration samples

Download from GitHub Releases: https://github.com/[your-org]/LucAstra/releases


## ðŸ“„ License

MIT License - See LICENSE file for details

## ðŸ™ Acknowledgments

- **Redox OS** - relibc compatibility layer
- **llamafile** - Portable LLM inference
- **iced** - Rust GUI framework
- **Rust community** - Amazing ecosystem and tools
